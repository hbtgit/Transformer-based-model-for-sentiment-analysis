# -*- coding: utf-8 -*-
"""Transformer_based_sentiment_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oD-9ujOrzmC_mrhFG8rVr6r_bvAlOt8P

# Installing transformer datasets
"""

!pip install torch transformers datasets

"""# Importing Libraries"""

import torch
import torch.nn as nn
import torch.optim as optim
import math
from datasets import load_dataset
from transformers import BertTokenizer
from torch.utils.data import DataLoader, Dataset
import numpy as np

"""# 1. Define the Positional Encoding"""

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :]
        return x

"""# 2. Define the Transformer Encoder Model for Classification"""

class TransformerClassifier(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward, max_seq_length, num_classes):
        super(TransformerClassifier, self).__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, max_seq_length)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)
        self.fc = nn.Linear(d_model, num_classes)
        self.dropout = nn.Dropout(0.1)

    def forward(self, src, src_mask=None):
        src = self.embedding(src) * math.sqrt(self.d_model)
        src = self.pos_encoder(src)
        output = self.transformer_encoder(src, src_mask)
        output = output.mean(dim=1)  # Global average pooling
        output = self.dropout(output)
        output = self.fc(output)
        return output

"""# 3. Custom Dataset Class for IMDb"""

class IMDbDataset(Dataset):
    def __init__(self, dataset, tokenizer, max_length):
        self.texts = dataset['text']
        self.labels = dataset['label']
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        encoding = self.tokenizer(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'label': torch.tensor(label, dtype=torch.long)
        }

"""# 4. Training Function

"""

def train_model(model, train_loader, device, epochs=3):
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            labels = batch['label'].to(device)

            optimizer.zero_grad()
            output = model(input_ids)
            loss = criterion(output, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
        print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')

"""# 5. Evaluation Function

"""

def evaluate_model(model, test_loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            labels = batch['label'].to(device)
            outputs = model(input_ids)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    accuracy = correct / total
    return accuracy

"""# 6. Main Execution

# Hyperparameters
"""

if __name__ == '__main__':
    # Hyperparameters
    vocab_size = 30522  # BERT tokenizer vocab size
    d_model = 128
    nhead = 4
    num_encoder_layers = 2
    dim_feedforward = 512
    max_seq_length = 128
    num_classes = 2
    batch_size = 32
    epochs = 3
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

"""# Load IMDb dataset"""

dataset = load_dataset('imdb')
    train_dataset = dataset['train']
    test_dataset = dataset['test']

"""# Initialize tokenizer"""

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

"""# Create data loaders"""

train_data = IMDbDataset(train_dataset, tokenizer, max_seq_length)
    test_data = IMDbDataset(test_dataset, tokenizer, max_seq_length)
    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_data, batch_size=batch_size)

"""# Initialize model"""

model = TransformerClassifier(
        vocab_size=vocab_size,
        d_model=d_model,
        nhead=nhead,
        num_encoder_layers=num_encoder_layers,
        dim_feedforward=dim_feedforward,
        max_seq_length=max_seq_length,
        num_classes=num_classes
    ).to(device)

"""# Train model"""

train_model(model, train_loader, device, epochs=epochs)

"""# Evaluate model"""

accuracy = evaluate_model(model, test_loader, device)
    print(f'Test Accuracy: {accuracy:.4f}')

"""# Example inference"""

sample_texts = [
        "This movie was absolutely fantastic! The acting was superb and the plot kept me engaged throughout.",
        "I hated this film. It was boring, poorly acted, and the story made no sense."
    ]
    model.eval()
    for text in sample_texts:
        encoding = tokenizer(
            text,
            add_special_tokens=True,
            max_length=max_seq_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        input_ids = encoding['input_ids'].to(device)
        with torch.no_grad():
            output = model(input_ids)
            pred = output.argmax(dim=1).item()
        print(f'Text: {text[:50]}..., Predicted Sentiment: {"Positive" if pred == 1 else "Negative"}')